{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0S3rKkx5XGvc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# from torchtext.datasets import TranslationDataset, Multi30k\n",
    "# from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# import spacy\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "0i_EH-Azg-9Y",
    "outputId": "3dfe2096-28f8-40a7-e4a7-79670dd6d287"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train.en'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-122ccd50bfd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_en\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain_de\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/train.en\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtrain_en\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/train.de\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train.en'"
     ]
    }
   ],
   "source": [
    "train_en = []\n",
    "train_de = []\n",
    "with open(\"data/train.en\", \"r\", encoding=\"utf8\") as f:\n",
    "    train_en = f.readlines()\n",
    "with open(\"data/train.de\", \"r\", encoding=\"utf8\") as f:\n",
    "    train_de = f.readlines()\n",
    "with open(\"data/dev.en\", \"r\", encoding=\"utf8\") as f:\n",
    "    dev_en = f.readlines()\n",
    "with open(\"data/dev.de\", \"r\", encoding=\"utf8\") as f:\n",
    "    dev_de = f.readlines()\n",
    "    \n",
    "print(len(train_de))\n",
    "print(len(train_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Eym-88ZSiUgF",
    "outputId": "6beb6b05-248a-4fc4-ad62-5f150ce0d49d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\evan_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Get rid of any white space or \\n's\n",
    "nltk.download('punkt')\n",
    "start = \"<sos>\"\n",
    "end = \"<eos>\"\n",
    "pad = \"<pad>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pNTBocP8iL-n"
   },
   "outputs": [],
   "source": [
    "start = \"<sos>\"\n",
    "end = \"<eos>\"\n",
    "pad = \"<pad>\"\n",
    "max_length_en = -1\n",
    "for i, sentence in enumerate(train_en):\n",
    "    sentence = nltk.word_tokenize(sentence.lower())\n",
    "    sentence = [start] + sentence\n",
    "    sentence.append(end)\n",
    "    max_length_en = len(sentence) if len(sentence) > max_length_en else max_length_en\n",
    "    train_en[i] = sentence\n",
    "    \n",
    "\n",
    "max_length_de = -1\n",
    "for i, sentence in enumerate(train_de):\n",
    "    sentence = nltk.word_tokenize(sentence.lower())\n",
    "    sentence = [start] + sentence\n",
    "    sentence.append(end)\n",
    "    max_length_de = len(sentence) if len(sentence) > max_length_de else max_length_de\n",
    "    train_de[i] = sentence\n",
    "    \n",
    "    \n",
    "max_length_en_dev = -1\n",
    "for i, sentence in enumerate(dev_en):\n",
    "    sentence = nltk.word_tokenize(sentence.lower())\n",
    "    sentence = [start] + sentence\n",
    "    sentence.append(end)\n",
    "    max_length_en_dev = len(sentence) if len(sentence) > max_length_en_dev else max_length_en_dev\n",
    "    dev_en[i] = sentence\n",
    "\n",
    "max_length_de_dev = -1\n",
    "for i, sentence in enumerate(dev_de):\n",
    "    sentence = nltk.word_tokenize(sentence.lower())\n",
    "    sentence = [start] + sentence\n",
    "    sentence.append(end)\n",
    "    max_length_de_dev = len(sentence) if len(sentence) > max_length_de_dev else max_length_de_dev\n",
    "    dev_de[i] = sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============== WARNING ===============\n",
    "\n",
    "## The following code cell deletes a number of sentences from the training set in order to get the dimensionality of the sentences below a certain threshold. The threshold is the value in the inequality of the while loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the longest sentences from the training dataset to decrease the dimensionality of all the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141442 743 768\n",
      "18280 731 612\n",
      "182228 626 612\n",
      "142855 618 612\n",
      "133057 591 581\n",
      "140860 561 563\n",
      "33569 510 563\n",
      "11402 509 498\n",
      "39936 488 498\n",
      "137934 478 438\n",
      "182252 465 438\n",
      "39916 436 438\n",
      "51490 431 438\n",
      "76485 420 415\n",
      "76489 411 415\n",
      "186741 403 415\n",
      "38928 381 415\n",
      "25430 368 412\n",
      "115778 365 412\n",
      "14016 360 412\n",
      "182246 360 412\n",
      "137574 358 412\n",
      "14189 355 412\n",
      "121443 355 412\n",
      "1858 342 369\n",
      "107941 336 323\n",
      "133054 316 323\n",
      "182160 306 323\n",
      "75136 304 323\n",
      "180891 299 323\n",
      "24491 298 299\n",
      "143305 297 299\n",
      "92319 269 267\n",
      "115807 257 267\n",
      "35723 256 267\n",
      "24491 251 267\n",
      "115776 250 264\n",
      "93534 248 264\n",
      "39953 244 264\n",
      "111570 243 264\n",
      "16453 233 264\n",
      "78572 230 264\n",
      "172799 229 264\n",
      "96738 224 224\n",
      "4101 221 224\n",
      "112667 216 224\n",
      "39896 212 205\n",
      "51092 212 205\n",
      "115785 210 205\n",
      "3589 203 202\n",
      "1233 194 202\n"
     ]
    }
   ],
   "source": [
    "max_idx, en_max_val = max(enumerate(train_en), key=lambda x: len(x[1]))\n",
    "_, de_max_val = max(enumerate(train_de), key=lambda x: len(x[1]))\n",
    "\n",
    "del_count = 0\n",
    "print(max_idx, len(en_max_val), len(de_max_val))\n",
    "\n",
    "while len(en_max_val) > 200:\n",
    "    del train_en[max_idx]\n",
    "    del train_de[max_idx]\n",
    "    \n",
    "    max_idx, en_max_val = max(enumerate(train_en), key=lambda x: len(x[1]))\n",
    "    _, de_max_val = max(enumerate(train_de), key=lambda x: len(x[1]))\n",
    "    \n",
    "    print(max_idx, len(en_max_val), len(de_max_val))\n",
    "    \n",
    "    del_count += 1\n",
    "    \n",
    "max_length_en = len(en_max_val)\n",
    "max_length_de = len(de_max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3fH-rhj1HOeE",
    "outputId": "67663a43-7c6e-480f-dd40-e589fe2ea4a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: 50\n",
      "Number of Sentences: 196834\n",
      "Max length English (training): 194\n",
      "Max length German (training): 202\n",
      "Max length English (dev): 173\n",
      "Max length German (dev): 169\n"
     ]
    }
   ],
   "source": [
    "print(\"Deleted:\", del_count)\n",
    "\n",
    "print(\"Number of Sentences:\", len(train_en))\n",
    "\n",
    "print(\"Max length English (training):\", max_length_en)\n",
    "print(\"Max length German (training):\", max_length_de)\n",
    "\n",
    "print(\"Max length English (dev):\", max_length_en_dev)\n",
    "print(\"Max length German (dev):\", max_length_de_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = {}\n",
    "de_vocab = {}\n",
    "en_vocab['<pad>'] = 0\n",
    "en_vocab['<unk>'] = 1 \n",
    "en_vocab['<sos>'] = 2\n",
    "en_vocab['<eos>'] = 3\n",
    "\n",
    "de_vocab['<pad>'] = 0\n",
    "de_vocab['<unk>'] = 1\n",
    "de_vocab['<sos>'] = 2\n",
    "de_vocab['<eos>'] = 3\n",
    "en_inputs = []\n",
    "de_inputs = []\n",
    "\n",
    "en_val = []\n",
    "de_val = []\n",
    "\n",
    "for sent in train_en:\n",
    "    en_idxes = []\n",
    "    for w in sent:\n",
    "        if w not in en_vocab:\n",
    "            en_vocab[w] = len(en_vocab)\n",
    "        en_idxes.append(en_vocab[w])\n",
    "    en_inputs.append(en_idxes)\n",
    "del train_en\n",
    "    \n",
    "for sent in train_de:\n",
    "    de_idxes = []\n",
    "    for w in sent:\n",
    "        if w not in de_vocab:\n",
    "            de_vocab[w] = len(de_vocab)\n",
    "        de_idxes.append(de_vocab[w])\n",
    "    de_inputs.append(de_idxes)\n",
    "del train_de\n",
    "    \n",
    "for sent in dev_en:\n",
    "    en_idxes = []\n",
    "    for w in sent:\n",
    "        if w not in en_vocab:\n",
    "            en_vocab[w] = len(en_vocab)\n",
    "        en_idxes.append(en_vocab[w])\n",
    "    en_val.append(en_idxes)\n",
    "del dev_en\n",
    "    \n",
    "for sent in dev_de:\n",
    "    de_idxes = []\n",
    "    for w in sent:\n",
    "        if w not in de_vocab:\n",
    "            de_vocab[w] = len(de_vocab)\n",
    "        de_idxes.append(de_vocab[w])\n",
    "    de_val.append(de_idxes)\n",
    "del dev_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(en_inputs):\n",
    "    diff = max_length_en - len(sentence)\n",
    "    if diff == 0:\n",
    "        continue\n",
    "    pad_array = [0]*diff\n",
    "    sentence = sentence + pad_array\n",
    "    en_inputs[i] = sentence\n",
    "\n",
    "for i, sentence in enumerate(de_inputs):\n",
    "    diff = max_length_de - len(sentence)\n",
    "    if diff == 0:\n",
    "        continue\n",
    "    pad_array = [0]*diff\n",
    "    sentence = sentence + pad_array\n",
    "    de_inputs[i] = sentence\n",
    "    \n",
    "for i, sentence in enumerate(en_val):\n",
    "    diff = max_length_en_dev - len(sentence)\n",
    "    if diff == 0:\n",
    "        continue\n",
    "    pad_array = [0]*diff\n",
    "    sentence = sentence + pad_array\n",
    "    en_val[i] = sentence\n",
    "\n",
    "for i, sentence in enumerate(de_val):\n",
    "    diff = max_length_de_dev - len(sentence)\n",
    "    if diff == 0:\n",
    "        continue\n",
    "    pad_array = [0]*diff\n",
    "    sentence = sentence + pad_array\n",
    "    de_val[i] = sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Warning ==========\n",
    "\n",
    "## The following code cell shrinks the number of training sentences from 196k to 50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "en_inputs = en_inputs[:50000]\n",
    "de_inputs = en_inputs[:50000]\n",
    "print(len(en_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_idx_to_word = dict((v,k) for k,v in en_vocab.items())\n",
    "de_idx_to_word = dict((v,k) for k,v in de_vocab.items())\n",
    "\n",
    "en_iwslt = {}\n",
    "de_iwslt = {}\n",
    "\n",
    "en_iwslt['idx2word'] = en_idx_to_word  \n",
    "de_iwslt['idx2word'] = de_idx_to_word\n",
    "\n",
    "en_iwslt['train'] = en_inputs\n",
    "de_iwslt['train'] = de_inputs\n",
    "\n",
    "en_iwslt['dev'] = en_val\n",
    "de_iwslt['dev'] = de_val\n",
    "\n",
    "with open('data/processed/english200_50k.pickle', 'wb') as handle:\n",
    "    pickle.dump(en_iwslt, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('data/processed/german200_50k.pickle', 'wb') as handle:\n",
    "    pickle.dump(de_iwslt, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 33, 12769, 3686, 20, 33, 117454, 68, 112324, 11, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "58365\n",
      "129067\n",
      "84\n",
      "<sos> als ich in meinen 20ern war , hatte ich meine erste psychotherapie-patientin . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(de_val[44])\n",
    "print(len(en_vocab))\n",
    "print(len(de_vocab))\n",
    "print(en_vocab['like'])\n",
    "\n",
    "for i in range(1): \n",
    "    for j in range(len(de_val[i])):\n",
    "        print(de_idx_to_word[de_val[i][j]], end=\" \")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OKqKmsVwoIp7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with open('data/processed/english200_50k.pickle', 'rb') as handle:\n",
    "    english = pickle.load(handle)\n",
    "    \n",
    "with open('data/processed/german200_50k.pickle', 'rb') as handle:\n",
    "    german = pickle.load(handle)\n",
    "    \n",
    "print(en_iwslt == english)\n",
    "print(de_iwslt == german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Preprocessing.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
